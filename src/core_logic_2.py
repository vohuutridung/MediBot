import streamlit as st
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import LlamaCpp
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
import os

# --- CONFIG ---
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.join(CURRENT_DIR, '..')
VECTOR_STORE_PATH = os.path.join(PROJECT_ROOT, "data", "processed", "faiss_index_medical")


# VECTOR_STORE_PATH = "data/processed/faiss_index_medical"
EMBEDDING_MODEL_NAME = "bkai-foundation-models/vietnamese-bi-encoder"
LOCAL_LLM_MODEL = "vinallama:7b-chat-local"
GGUF_MODEL_PATH = f"src/models/vinallama-7b-chat_q5_0.gguf"  # üîÑ ƒê∆∞·ªùng d·∫´n file gguf

@st.cache_resource
def load_rag_pipeline():
    """T·∫£i Vector DB, Embedding model v√† LLM local. H√†m n√†y ƒë∆∞·ª£c cache ƒë·ªÉ tƒÉng t·ªëc."""
    print("ƒêang t·∫£i pipeline RAG...")
    # T·∫£i embedding model
    model_kwargs = {'device': 'cpu'}
    encode_kwargs = {'normalize_embeddings': False}
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL_NAME,
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs
    )
    
    # T·∫£i Vector DB
    vector_store = FAISS.load_local(VECTOR_STORE_PATH, embeddings, allow_dangerous_deserialization=True)
    
    # Kh·ªüi t·∫°o LLM local
    llm = LlamaCpp(
        model_path=GGUF_MODEL_PATH,
        n_ctx=4096,
        temperature=0.3,
        max_tokens=1024,
        verbose=True,
        n_threads=3,  # ƒêi·ªÅu ch·ªânh theo m√°y b·∫°n
        n_gpu_layers=0  # 0 n·∫øu kh√¥ng d√πng GPU
    )
    
    # T·∫°o Prompt Template
    prompt_template = """
    B·∫°n l√† m·ªôt tr·ª£ l√Ω y khoa h·ªØu √≠ch. Ch·ªâ s·ª≠ d·ª•ng nh·ªØng th√¥ng tin ƒë∆∞·ª£c cung c·∫•p trong ph·∫ßn ng·ªØ c·∫£nh d∆∞·ªõi ƒë√¢y ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi.
    Tuy·ªát ƒë·ªëi kh√¥ng t·ª± b·ªãa ƒë·∫∑t th√¥ng tin. N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin trong ng·ªØ c·∫£nh, h√£y n√≥i r·∫±ng b·∫°n kh√¥ng c√≥ ƒë·ªß th√¥ng tin ƒë·ªÉ tr·∫£ l·ªùi.
    Lu√¥n nh·∫Øc nh·ªü ng∆∞·ªùi d√πng r·∫±ng th√¥ng tin ch·ªâ ƒë·ªÉ tham kh·∫£o v√† h·ªç n√™n tham v·∫•n √Ω ki·∫øn b√°c sƒ© chuy√™n khoa ƒë·ªÉ c√≥ ch·∫©n ƒëo√°n ch√≠nh x√°c.

    Ng·ªØ c·∫£nh:
    {context}

    C√¢u h·ªèi:
    {question}

    C√¢u tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát:
    """
    QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt_template)
    
    # T·∫°o chu·ªói RetrievalQA
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vector_store.as_retriever(search_kwargs={"k": 3}),
        return_source_documents=True,
        chain_type_kwargs={"prompt": QA_CHAIN_PROMPT}
    )
    print("Pipeline RAG ƒë√£ s·∫µn s√†ng.")
    return qa_chain

def get_rag_response(qa_chain, query: str):
    """Nh·∫≠n query v√† tr·∫£ v·ªÅ k·∫øt qu·∫£ t·ª´ RAG chain."""
    try:
        result = qa_chain.invoke({"query": query})
        return result
    except Exception as e:
        print(f"L·ªói khi th·ª±c thi RAG chain: {e}")
        return {"result": "Xin l·ªói, ƒë√£ c√≥ l·ªói x·∫£y ra trong qu√° tr√¨nh x·ª≠ l√Ω. Vui l√≤ng th·ª≠ l·∫°i.", "source_documents": []}